# Intrusion Detecting System for CAN Bus

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.13+-orange.svg)](https://www.tensorflow.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

##  Adversarially Robust Deep Learning IDS for Controller Area Network

An advanced intrusion detection system designed to identify and defend against cyberattacks on CAN bus networks in connected and autonomous vehicles. This system combines deep learning with adversarial robustness techniques to achieve state-of-the-art detection performance.

---

## ðŸŽ¯ Key Features

### ðŸ”’ **Adversarial Robustness**
- **FGSM, PGD, and C&W Attacks**: Defense against gradient-based adversarial perturbations
- **Automotive-Specific Attacks**: Resistance to CAN bus injection and spoofing attacks
- **Adversarial Training**: Robust model training with multiple attack types
- **Robustness Score**: 78-88% resistance to adversarial attacks

### ðŸš— **Cross-Vehicle Generalization**
- **Domain Adversarial Neural Networks (DANN)**: Transfer learning across vehicle models
- **Multi-Vehicle Training**: Single model works across different CAN protocols
- **Few-Shot Adaptation**: Quick adaptation to new vehicle types

### ðŸ“¦ **Lightweight & Deployable**
- **Model Compression**: Int8 quantization (4 MB â†’ 1 MB, 75% reduction)
- **TensorFlow Lite**: Ready for embedded systems and edge devices
- **Real-Time Performance**: <10ms inference time
- **Low Resource Usage**: Optimized for 8GB RAM systems

### ðŸŽ¯ **Uncertainty Quantification**
- **Monte Carlo Dropout**: Confidence estimation for predictions
- **Ensemble Methods**: Multiple models for robust detection
- **Calibrated Confidence**: Trustworthy anomaly scores

---

## ðŸ“Š Performance

### Attack Detection Results

| Attack Type | F1-Score | TPR | FPR |
|-------------|----------|-----|-----|
| **Flooding** | 0.91 | 0.88 | 0.009 |
| **Suppress** | 0.89 | 0.85 | 0.011 |
| **Plateau** | 0.92 | 0.90 | 0.008 |
| **Continuous** | 0.88 | 0.84 | 0.010 |
| **Playback** | 0.93 | 0.91 | 0.007 |
| **Average** | **0.91** | **0.88** | **0.009** |

### Robustness Metrics
- **Adversarial Robustness**: 78-88%
- **False Positive Rate**: <1%
- **Inference Time**: 8.5 ms
- **Model Size**: 1 MB (compressed)

---

## ðŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/sarthakporwal/ids.git
cd ids

# Create virtual environment
python3 -m venv canshield_env
source canshield_env/bin/activate  # On Windows: canshield_env\Scripts\activate

# Install dependencies
pip install tensorflow==2.13.0 keras==2.13.1
pip install tensorflow-model-optimization
pip install hydra-core scikit-learn pandas numpy matplotlib seaborn
```

### Visualization

```bash
# Generate graphs and plots
python visualize_results.py

# View results
open artifacts/visualizations/
```

---

## ðŸ—ï¸ Architecture

### Model Components

```
Input (CAN Signals)
    â†“
Signal Preprocessing
    â†“
Autoencoder Network (37,825 params)
    â”œâ”€ Encoder: Extract features
    â””â”€ Decoder: Reconstruct signals
    â†“
Reconstruction Error
    â†“
Threshold-Based Detection
    â†“
Attack Classification
```

### Enhanced Features

1. **Adversarial Training Module**
   - FGSM attack generation
   - PGD attack generation
   - Automotive-specific attacks
   - Multi-attack training

2. **Domain Adaptation Module**
   - Transfer learning
   - Multi-vehicle training
   - Few-shot adaptation

3. **Model Compression Module**
   - Quantization (Int8, Float16)
   - Pruning
   - Knowledge distillation
   - TFLite conversion

4. **Uncertainty Estimation Module**
   - Monte Carlo Dropout
   - Ensemble uncertainty
   - Confidence calibration

---
---

## ðŸŽ¨ Visualizations

The system generates professional visualizations:

### Training History
![Training Curves](artifacts/visualizations/training_history.png)

### Performance Summary
![Summary Report](artifacts/visualizations/summary_report.png)

### Loss Heatmap
![Loss Heatmap](artifacts/visualizations/loss_heatmap.png)

---

## ðŸ”¬ Research & Technical Details

### Adversarial Robustness
- **FGSM (Fast Gradient Sign Method)**: One-step gradient attack
- **PGD (Projected Gradient Descent)**: Multi-step iterative attack
- **Automotive Attacks**: CAN-specific injection and timing attacks

### Training Strategy
- **Curriculum Learning**: Gradual epsilon scheduling
- **Mixed Batch Training**: 25% clean + 75% adversarial examples
- **Early Stopping**: Validation-based convergence

### Optimization
- Memory-optimized for 8GB RAM systems
- Configurable batch size and window steps
- GPU acceleration support (Metal, CUDA)

---


- **[FINAL_OUTPUT_GUIDE.md](FINAL_OUTPUT_GUIDE.md)** - Understanding your results



## ðŸ“Š Comparison with State-of-the-Art

| Model | F1-Score | Robustness | Size | Speed |
|-------|----------|------------|------|-------|
| **This IDS** | **0.91** | **0.78-0.88** | **1 MB** | **<10ms** |
| CANet | 0.95 | N/A | 15 MB | ~50ms |
| LSTM-Based | 0.87 | 0.65 | 8 MB | ~30ms |
| CNN-Based | 0.89 | 0.70 | 12 MB | ~25ms |

---

##  Acknowledgments

This project builds upon:
- **SynCAN Dataset**: [ETAS SynCAN](https://github.com/etas/SynCAN)
- **TensorFlow & Keras**: Deep learning frameworks

---

## ðŸ“ˆ Future Work

- [ ] Support for additional CAN datasets (ROAD, Car-Hacking)
- [ ] Mobile deployment (iOS/Android)
